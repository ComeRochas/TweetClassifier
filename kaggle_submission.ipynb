{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055422ea",
   "metadata": {},
   "source": [
    "# Entraînement et Soumission Kaggle - Modèle Multimodal\n",
    "\n",
    "Ce notebook regroupe l'ensemble du pipeline pour entraîner un modèle de classification de tweets (multimodal : texte + métadonnées) et générer un fichier de soumission pour Kaggle.\n",
    "\n",
    "## 1. Importations et Configuration de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80657794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# Configuration du device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Fixation de la graine aléatoire pour la reproductibilité\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb270f4b",
   "metadata": {},
   "source": [
    "## 2. Configuration des Hyperparamètres\n",
    "\n",
    "Nous définissons ici une classe de configuration pour centraliser tous les hyperparamètres du modèle et de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27007d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    # Modèle\n",
    "    transformer_name: str = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    metadata_dim: int = 8\n",
    "    meta_hidden_dim: int = 32\n",
    "    text_hidden_dim: int = 768\n",
    "    fusion_hidden_dim: int = 256\n",
    "    \n",
    "    # Entraînement\n",
    "    max_length: int = 160\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 4\n",
    "    lr_transformer: float = 2e-5\n",
    "    lr_head: float = 1e-3\n",
    "    weight_decay: float = 0.01\n",
    "    seed: int = 42\n",
    "    freeze_transformer: bool = False\n",
    "\n",
    "cfg = TrainingConfig()\n",
    "print(\"Configuration actuelle :\")\n",
    "pprint.pprint(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef60af",
   "metadata": {},
   "source": [
    "## 3. Chargement et Prétraitement des Données\n",
    "\n",
    "Cette section contient les fonctions pour charger les fichiers JSONL, extraire les caractéristiques textuelles et les métadonnées, et normaliser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4505fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_row(row):\n",
    "    \"\"\"\n",
    "    Extrait le texte complet et les métadonnées d'une ligne de données (tweet).\n",
    "    \"\"\"\n",
    "    # 1. Extraction du texte complet\n",
    "    text = row.get('text', '')\n",
    "    # Gestion des tweets étendus (si json_normalize a aplati la structure)\n",
    "    if pd.notna(row.get('extended_tweet.full_text')):\n",
    "        text = row['extended_tweet.full_text']\n",
    "        \n",
    "    # Normalisation du texte et ajout de l'URL utilisateur\n",
    "    text = str(text).replace('\\n', ' ').strip()\n",
    "    user_url = row.get('user.url', '')\n",
    "    if pd.notna(user_url) and user_url:\n",
    "        text = f\"[TEXTE] {text} [PERSONAL_URL] {user_url}\"\n",
    "    else:\n",
    "        text = f\"[TEXTE] {text}\"\n",
    "        \n",
    "    # 2. Extraction des métadonnées\n",
    "    def safe_get(key, default=0):\n",
    "        val = row.get(key, default)\n",
    "        if pd.isna(val):\n",
    "            return default\n",
    "        return val\n",
    "\n",
    "    metadata = {\n",
    "        'user_default_profile': int(safe_get('user.default_profile', False)),\n",
    "        'user_profile_use_background_image': int(safe_get('user.profile_use_background_image', False)),\n",
    "        'user_statuses_count': float(safe_get('user.statuses_count', 0)),\n",
    "        'user_profile_background_tile': int(safe_get('user.profile_background_tile', False)),\n",
    "        'user_geo_enabled': int(safe_get('user.geo_enabled', False)),\n",
    "        'user_is_translator': int(safe_get('user.is_translator', False)),\n",
    "        'user_favourites_count': float(safe_get('user.favourites_count', 0)),\n",
    "        'user_listed_count': float(safe_get('user.listed_count', 0))\n",
    "    }\n",
    "    \n",
    "    return pd.Series({'full_text': text, **metadata})\n",
    "\n",
    "def load_data(train_path='train.jsonl', test_path='kaggle_test.jsonl'):\n",
    "    # Chargement Train\n",
    "    print(f\"Chargement des données d'entraînement depuis {train_path}...\")\n",
    "    train_data_raw = []\n",
    "    if os.path.exists(train_path):\n",
    "        with open(train_path, 'r') as f:\n",
    "            for line in f:\n",
    "                train_data_raw.append(json.loads(line))\n",
    "        df_train = pd.json_normalize(train_data_raw)\n",
    "    else:\n",
    "        print(f\"Attention: {train_path} introuvable.\")\n",
    "        df_train = pd.DataFrame()\n",
    "    \n",
    "    # Chargement Test (Kaggle)\n",
    "    print(f\"Chargement des données de test depuis {test_path}...\")\n",
    "    test_data_raw = []\n",
    "    if os.path.exists(test_path):\n",
    "        with open(test_path, 'r') as f:\n",
    "            for line in f:\n",
    "                test_data_raw.append(json.loads(line))\n",
    "        df_test = pd.json_normalize(test_data_raw)\n",
    "    else:\n",
    "        print(f\"Attention: {test_path} introuvable.\")\n",
    "        df_test = pd.DataFrame()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def preprocess_data(df_train, df_test):\n",
    "    # Extraction des features\n",
    "    print(\"Extraction des features pour le train...\")\n",
    "    train_features = df_train.apply(extract_features_from_row, axis=1)\n",
    "    \n",
    "    print(\"Extraction des features pour le test...\")\n",
    "    test_features = df_test.apply(extract_features_from_row, axis=1)\n",
    "    \n",
    "    # Transformation Logarithmique pour certaines colonnes numériques\n",
    "    log_cols = ['user_statuses_count', 'user_favourites_count', 'user_listed_count']\n",
    "    for col in log_cols:\n",
    "        train_features[col] = np.log1p(train_features[col])\n",
    "        test_features[col] = np.log1p(test_features[col])\n",
    "        \n",
    "    # Séparation des métadonnées pour le scaling\n",
    "    metadata_cols = [c for c in train_features.columns if c != 'full_text']\n",
    "    \n",
    "    # Scaling (StandardScaler)\n",
    "    print(\"Mise à l'échelle des métadonnées...\")\n",
    "    scaler = StandardScaler()\n",
    "    # Fit sur le train uniquement\n",
    "    metadata_train = scaler.fit_transform(train_features[metadata_cols].values)\n",
    "    # Transform sur le test\n",
    "    metadata_test = scaler.transform(test_features[metadata_cols].values)\n",
    "    \n",
    "    # Préparation des listes finales de dictionnaires\n",
    "    train_list = []\n",
    "    for i in range(len(train_features)):\n",
    "        train_list.append({\n",
    "            \"text\": train_features.iloc[i]['full_text'],\n",
    "            \"metadata\": metadata_train[i],\n",
    "            \"label\": df_train.iloc[i]['label']\n",
    "        })\n",
    "        \n",
    "    test_list = []\n",
    "    for i in range(len(test_features)):\n",
    "        test_list.append({\n",
    "            \"text\": test_features.iloc[i]['full_text'],\n",
    "            \"metadata\": metadata_test[i],\n",
    "            \"challenge_id\": df_test.iloc[i]['challenge_id']\n",
    "        })\n",
    "        \n",
    "    return train_list, test_list, scaler\n",
    "\n",
    "# Exécution du chargement (si les fichiers sont présents)\n",
    "if os.path.exists('train.jsonl') and os.path.exists('kaggle_test.jsonl'):\n",
    "    df_train, df_test = load_data()\n",
    "    train_data_processed, kaggle_data_processed, scaler = preprocess_data(df_train, df_test)\n",
    "    print(f\"Données traitées : {len(train_data_processed)} exemples d'entraînement, {len(kaggle_data_processed)} exemples de test.\")\n",
    "else:\n",
    "    print(\"Fichiers de données non trouvés dans le répertoire courant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb0cea",
   "metadata": {},
   "source": [
    "## 4. Classe Dataset PyTorch\n",
    "\n",
    "Définition de la classe `TweetDataset` pour gérer les données textuelles et numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, with_labels=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of dict): Liste d'échantillons.\n",
    "            tokenizer: Tokenizer HuggingFace.\n",
    "            max_length (int): Longueur maximale de la séquence.\n",
    "            with_labels (bool): Si True, retourne aussi les labels.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.with_labels = with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        metadata = item[\"metadata\"]\n",
    "\n",
    "        # Tokenisation\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Suppression de la dimension de batch ajoutée par le tokenizer (1, seq_len) -> (seq_len)\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Conversion des métadonnées en tenseur float\n",
    "        metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"metadata\": metadata_tensor\n",
    "        }\n",
    "\n",
    "        if self.with_labels:\n",
    "            label = item[\"label\"]\n",
    "            result[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3a6b8",
   "metadata": {},
   "source": [
    "## 5. Architecture du Modèle Multimodal\n",
    "\n",
    "Le modèle combine un Transformer (pour le texte) et un MLP (pour les métadonnées)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTweetClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 transformer_name=\"cardiffnlp/twitter-xlm-roberta-base\",\n",
    "                 metadata_dim=8,\n",
    "                 text_hidden_dim=768,\n",
    "                 meta_hidden_dim=32,\n",
    "                 fusion_hidden_dim=256):\n",
    "        super(MultimodalTweetClassifier, self).__init__()\n",
    "        \n",
    "        # 1. Encodeur Transformer\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        \n",
    "        # 2. MLP pour les métadonnées\n",
    "        self.meta_mlp = nn.Sequential(\n",
    "            nn.Linear(metadata_dim, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(meta_hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # 3. Classifieur de fusion\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(text_hidden_dim + meta_hidden_dim, fusion_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(fusion_hidden_dim, 2)  # 2 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, metadata):\n",
    "        # Passage du texte dans le transformer\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Récupération du token CLS (premier token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Passage des métadonnées dans le MLP\n",
    "        meta_repr = self.meta_mlp(metadata)\n",
    "        \n",
    "        # Concaténation\n",
    "        fused = torch.cat([cls_embedding, meta_repr], dim=1)\n",
    "        \n",
    "        # Classification finale\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca455741",
   "metadata": {},
   "source": [
    "## 6. Fonctions d'Entraînement et de Validation\n",
    "\n",
    "Fonctions utilitaires pour exécuter une époque d'entraînement et évaluer le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        metadata = batch['metadata'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, metadata)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, metadata)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfef8a",
   "metadata": {},
   "source": [
    "## 7. Pipeline Principal d'Entraînement\n",
    "\n",
    "Lancement de l'entraînement avec séparation train/validation, initialisation du modèle et boucle d'époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que les données sont chargées\n",
    "if 'train_data_processed' in locals():\n",
    "    # Séparation Train / Validation\n",
    "    train_list, val_list = train_test_split(\n",
    "        train_data_processed, \n",
    "        test_size=0.2, \n",
    "        random_state=cfg.seed, \n",
    "        stratify=[x['label'] for x in train_data_processed]\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_list)}, Val size: {len(val_list)}\")\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.transformer_name)\n",
    "\n",
    "    # Datasets & DataLoaders\n",
    "    train_dataset = TweetDataset(train_list, tokenizer, cfg.max_length, with_labels=True)\n",
    "    val_dataset = TweetDataset(val_list, tokenizer, cfg.max_length, with_labels=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # Initialisation du Modèle\n",
    "    model = MultimodalTweetClassifier(\n",
    "        transformer_name=cfg.transformer_name,\n",
    "        metadata_dim=cfg.metadata_dim,\n",
    "        text_hidden_dim=cfg.text_hidden_dim,\n",
    "        meta_hidden_dim=cfg.meta_hidden_dim,\n",
    "        fusion_hidden_dim=cfg.fusion_hidden_dim\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optionnel : Geler le transformer\n",
    "    if cfg.freeze_transformer:\n",
    "        print(\"Gel des paramètres du Transformer...\")\n",
    "        for p in model.transformer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Optimiseur\n",
    "    # On applique des learning rates différents pour le transformer et la tête de classification\n",
    "    transformer_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "    param_groups = []\n",
    "    if len(transformer_params) > 0:\n",
    "        param_groups.append({'params': transformer_params, 'lr': cfg.lr_transformer})\n",
    "    param_groups.append({'params': model.meta_mlp.parameters(), 'lr': cfg.lr_head})\n",
    "    param_groups.append({'params': model.classifier.parameters(), 'lr': cfg.lr_head})\n",
    "\n",
    "    optimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Boucle d'Entraînement\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(\"Début de l'entraînement...\")\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n",
    "            print(\"--> Nouveau meilleur modèle sauvegardé.\")\n",
    "else:\n",
    "    print(\"Données non chargées, impossible de lancer l'entraînement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4cdb8b",
   "metadata": {},
   "source": [
    "## 8. Inférence et Génération du Fichier de Soumission\n",
    "\n",
    "Chargement du meilleur modèle et prédiction sur le jeu de test Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a011e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_data_processed' in locals() and os.path.exists(\"best_multimodal_model.pt\"):\n",
    "    # Chargement du meilleur modèle\n",
    "    print(\"Chargement du meilleur modèle pour l'inférence...\")\n",
    "    model.load_state_dict(torch.load(\"best_multimodal_model.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Dataset Kaggle\n",
    "    kaggle_dataset = TweetDataset(kaggle_data_processed, tokenizer, cfg.max_length, with_labels=False)\n",
    "    kaggle_loader = DataLoader(kaggle_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    print(\"Lancement des prédictions sur le jeu de test Kaggle...\")\n",
    "    with torch.no_grad():\n",
    "        batch_start_idx = 0\n",
    "        for batch in tqdm(kaggle_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, metadata)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            \n",
    "            # Récupération des IDs correspondants\n",
    "            batch_size = input_ids.size(0)\n",
    "            batch_ids = [item['challenge_id'] for item in kaggle_data_processed[batch_start_idx : batch_start_idx + batch_size]]\n",
    "            all_ids.extend(batch_ids)\n",
    "            \n",
    "            batch_start_idx += batch_size\n",
    "\n",
    "    # Création du DataFrame de soumission\n",
    "    results_df = pd.DataFrame({\n",
    "        \"ID\": all_ids,\n",
    "        \"Prediction\": all_preds\n",
    "    })\n",
    "\n",
    "    output_file = \"submission.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Prédictions sauvegardées dans {output_file}\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"Impossible de lancer l'inférence (données manquantes ou modèle non entraîné).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
