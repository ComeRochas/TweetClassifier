{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e356ce5",
   "metadata": {
    "papermill": {
     "duration": 0.00352,
     "end_time": "2025-12-04T16:10:46.906053",
     "exception": false,
     "start_time": "2025-12-04T16:10:46.902533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Entraînement et Soumission Kaggle - Modèle Multimodal\n",
    "\n",
    "Ce notebook regroupe l'ensemble du pipeline pour entraîner un modèle de classification de tweets (multimodal : texte + métadonnées) et générer un fichier de soumission pour Kaggle.\n",
    "\n",
    "## 1. Importations et Configuration de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24703e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:10:46.912489Z",
     "iopub.status.busy": "2025-12-04T16:10:46.912222Z",
     "iopub.status.idle": "2025-12-04T16:11:01.211848Z",
     "shell.execute_reply": "2025-12-04T16:11:01.210992Z"
    },
    "papermill": {
     "duration": 14.304398,
     "end_time": "2025-12-04T16:11:01.213111",
     "exception": false,
     "start_time": "2025-12-04T16:10:46.908713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# Configuration du device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Fixation de la graine aléatoire pour la reproductibilité\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "train_path=\"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025/train.jsonl\"\n",
    "test_path='/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025/kaggle_test.jsonl'\n",
    "\n",
    "print(os.path.exists(train_path) and os.path.exists(test_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911a087",
   "metadata": {
    "papermill": {
     "duration": 0.002581,
     "end_time": "2025-12-04T16:11:01.218648",
     "exception": false,
     "start_time": "2025-12-04T16:11:01.216067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configuration des Hyperparamètres\n",
    "\n",
    "Nous définissons ici une classe de configuration pour centraliser tous les hyperparamètres du modèle et de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd413d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:11:01.224694Z",
     "iopub.status.busy": "2025-12-04T16:11:01.224335Z",
     "iopub.status.idle": "2025-12-04T16:11:01.229705Z",
     "shell.execute_reply": "2025-12-04T16:11:01.228981Z"
    },
    "papermill": {
     "duration": 0.00975,
     "end_time": "2025-12-04T16:11:01.230877",
     "exception": false,
     "start_time": "2025-12-04T16:11:01.221127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration actuelle :\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    # Modèle\n",
    "    transformer_name = '/kaggle/input/twitterroberta/transformers/default/1/twitter-xlm-roberta-base'\n",
    "\n",
    "    metadata_dim: int = 8\n",
    "    meta_hidden_dim: int = 32\n",
    "    text_hidden_dim: int = 768\n",
    "    fusion_hidden_dim: int = 256\n",
    "    \n",
    "    # Entraînement\n",
    "    max_length: int = 220 #vérifier quelle doit être la bonne longueur mtn que j'ajoute la description et le nom du site\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 4\n",
    "    lr_transformer: float = 2e-5\n",
    "    lr_head: float = 1e-3\n",
    "    weight_decay: float = 0.01\n",
    "    seed: int = 42\n",
    "    freeze_transformer: bool = False\n",
    "\n",
    "cfg = TrainingConfig()\n",
    "print(\"Configuration actuelle :\")\n",
    "pprint.pprint(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73627cdc",
   "metadata": {
    "papermill": {
     "duration": 0.002585,
     "end_time": "2025-12-04T16:11:01.236104",
     "exception": false,
     "start_time": "2025-12-04T16:11:01.233519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Chargement et Prétraitement des Données\n",
    "\n",
    "Cette section contient les fonctions pour charger les fichiers JSONL, extraire les caractéristiques textuelles et les métadonnées, et normaliser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93559f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:11:01.242533Z",
     "iopub.status.busy": "2025-12-04T16:11:01.242273Z",
     "iopub.status.idle": "2025-12-04T16:13:39.893084Z",
     "shell.execute_reply": "2025-12-04T16:13:39.892341Z"
    },
    "papermill": {
     "duration": 158.658371,
     "end_time": "2025-12-04T16:13:39.897048",
     "exception": false,
     "start_time": "2025-12-04T16:11:01.238677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025/train.jsonl\n",
      "Chargement des données d'entraînement depuis /kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025/train.jsonl...\n",
      "Chargement des données de test depuis /kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025/kaggle_test.jsonl...\n",
      "Extraction des features pour le train...\n",
      "Extraction des features pour le test...\n",
      "Mise à l'échelle des métadonnées...\n",
      "Données traitées : 154914 exemples d'entraînement, 103380 exemples de test.\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_row(row):\n",
    "    \"\"\"\n",
    "    Extrait le texte complet et les métadonnées d'une ligne de données (tweet).\n",
    "    \"\"\"\n",
    "    # 1. Extraction du texte complet\n",
    "    text = row.get('text', '')\n",
    "    # Gestion des tweets étendus (si json_normalize a aplati la structure)\n",
    "    if pd.notna(row.get('extended_tweet.full_text')):\n",
    "        text = row['extended_tweet.full_text']\n",
    "        \n",
    "    # Normalisation du texte et ajout de l'URL utilisateur et de sa bio\n",
    "    text = str(text).replace('\\n', ' ').strip()\n",
    "    user_url = row.get('user.url', '')\n",
    "    user_description = row.get('user.description', '')\n",
    "    quoted_status_full_text = row.get('quoted_status.full_text', '')\n",
    "    text = f\"[TEXTE] {text}\"\n",
    "    if pd.notna(user_description) and user_description:\n",
    "        text = f\"{text} [PERSONAL_BIO] {user_description}\"\n",
    "    if pd.notna(user_url) and user_url:\n",
    "        text = f\"{text} [PERSONAL_URL] {user_url}\"\n",
    "    #if pd.notna(quoted_status_full_text) and quoted_status_full_text:\n",
    "    #    text = f\"{text} [QUOTED_STATUS] {quoted_status_full_text}\"\n",
    "    #unquote later mais je veux tester le nombre de features d'abord\n",
    "    \n",
    "\n",
    "        \n",
    "    # 2. Extraction des métadonnées\n",
    "    def safe_get(key, default=0):\n",
    "        val = row.get(key, default)\n",
    "        if pd.isna(val):\n",
    "            return default\n",
    "        return val\n",
    "\n",
    "    metadata = {\n",
    "        'user_default_profile': int(safe_get('user.default_profile', False)),\n",
    "        'user_profile_use_background_image': int(safe_get('user.profile_use_background_image', False)),\n",
    "        'user_statuses_count': float(safe_get('user.statuses_count', 0)),\n",
    "        'user_profile_background_tile': int(safe_get('user.profile_background_tile', False)),\n",
    "        'user_geo_enabled': int(safe_get('user.geo_enabled', False)),\n",
    "        'user_is_translator': int(safe_get('user.is_translator', False)),\n",
    "        'user_favourites_count': float(safe_get('user.favourites_count', 0)),\n",
    "        'user_listed_count': float(safe_get('user.listed_count', 0)),\n",
    "        'quoted_is_quote_status': int(safe_get('quoted_status.is_quote_status', False)),\n",
    "        'quoted_favorite_count': float(safe_get('quoted_status.favorite_count', 0)),\n",
    "        'quoted_quote_count': float(safe_get('quoted_status.quote_count', 0)),\n",
    "        'quoted_possibly_sensitive': int(safe_get('quoted_status.possibly_sensitive', False)),\n",
    "        'quoted_user_listed_count': float(safe_get('quoted_status.user.listed_count', 0)),\n",
    "    }\n",
    "    \n",
    "    return pd.Series({'full_text': text, **metadata})\n",
    "\n",
    "def load_data():\n",
    "    # Chargement Train\n",
    "    print(f\"Chargement des données d'entraînement depuis {train_path}...\")\n",
    "    train_data_raw = []\n",
    "    if os.path.exists(train_path):\n",
    "        with open(train_path, 'r') as f:\n",
    "            for line in f:\n",
    "                train_data_raw.append(json.loads(line))\n",
    "        df_train = pd.json_normalize(train_data_raw)\n",
    "    else:\n",
    "        print(f\"Attention: {train_path} introuvable.\")\n",
    "        df_train = pd.DataFrame()\n",
    "    \n",
    "    # Chargement Test (Kaggle)\n",
    "    print(f\"Chargement des données de test depuis {test_path}...\")\n",
    "    test_data_raw = []\n",
    "    if os.path.exists(test_path):\n",
    "        with open(test_path, 'r') as f:\n",
    "            for line in f:\n",
    "                test_data_raw.append(json.loads(line))\n",
    "        df_test = pd.json_normalize(test_data_raw)\n",
    "    else:\n",
    "        print(f\"Attention: {test_path} introuvable.\")\n",
    "        df_test = pd.DataFrame()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def preprocess_data(df_train, df_test):\n",
    "    # Extraction des features\n",
    "    print(\"Extraction des features pour le train...\")\n",
    "    train_features = df_train.apply(extract_features_from_row, axis=1)\n",
    "    \n",
    "    print(\"Extraction des features pour le test...\")\n",
    "    test_features = df_test.apply(extract_features_from_row, axis=1)\n",
    "    \n",
    "    # Transformation Logarithmique pour certaines colonnes numériques\n",
    "    log_cols = [\n",
    "        'user_statuses_count', \n",
    "        'user_favourites_count', \n",
    "        'user_listed_count',\n",
    "        'quoted_favorite_count',\n",
    "        'quoted_quote_count',\n",
    "        'quoted_user_listed_count'\n",
    "    ]\n",
    "    for col in log_cols:\n",
    "        train_features[col] = np.log1p(train_features[col])\n",
    "        test_features[col] = np.log1p(test_features[col])\n",
    "        \n",
    "    # Séparation des métadonnées pour le scaling\n",
    "    metadata_cols = [c for c in train_features.columns if c != 'full_text']\n",
    "    \n",
    "    # Scaling (StandardScaler)\n",
    "    print(\"Mise à l'échelle des métadonnées...\")\n",
    "    scaler = StandardScaler()\n",
    "    # Fit sur le train uniquement\n",
    "    metadata_train = scaler.fit_transform(train_features[metadata_cols].values)\n",
    "    # Transform sur le test\n",
    "    metadata_test = scaler.transform(test_features[metadata_cols].values)\n",
    "    \n",
    "    # Préparation des listes finales de dictionnaires\n",
    "    train_list = []\n",
    "    for i in range(len(train_features)):\n",
    "        train_list.append({\n",
    "            \"text\": train_features.iloc[i]['full_text'],\n",
    "            \"metadata\": metadata_train[i],\n",
    "            \"label\": df_train.iloc[i]['label']\n",
    "        })\n",
    "        \n",
    "    test_list = []\n",
    "    for i in range(len(test_features)):\n",
    "        test_list.append({\n",
    "            \"text\": test_features.iloc[i]['full_text'],\n",
    "            \"metadata\": metadata_test[i],\n",
    "            \"challenge_id\": df_test.iloc[i]['challenge_id']\n",
    "        })\n",
    "        \n",
    "    return train_list, test_list, scaler\n",
    "\n",
    "# Exécution du chargement (si les fichiers sont présents)\n",
    "print(train_path)\n",
    "if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "    df_train, df_test = load_data()\n",
    "    train_data_processed, kaggle_data_processed, scaler = preprocess_data(df_train, df_test)\n",
    "    print(f\"Données traitées : {len(train_data_processed)} exemples d'entraînement, {len(kaggle_data_processed)} exemples de test.\")\n",
    "else:\n",
    "    print(\"Fichiers de données non trouvés dans le répertoire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387947f",
   "metadata": {
    "papermill": {
     "duration": 0.002656,
     "end_time": "2025-12-04T16:13:39.902512",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.899856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Classe Dataset PyTorch\n",
    "\n",
    "Définition de la classe `TweetDataset` pour gérer les données textuelles et numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75133fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:13:39.908961Z",
     "iopub.status.busy": "2025-12-04T16:13:39.908713Z",
     "iopub.status.idle": "2025-12-04T16:13:39.914818Z",
     "shell.execute_reply": "2025-12-04T16:13:39.914309Z"
    },
    "papermill": {
     "duration": 0.010661,
     "end_time": "2025-12-04T16:13:39.915855",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.905194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, with_labels=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of dict): Liste d'échantillons.\n",
    "            tokenizer: Tokenizer HuggingFace.\n",
    "            max_length (int): Longueur maximale de la séquence.\n",
    "            with_labels (bool): Si True, retourne aussi les labels.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.with_labels = with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        metadata = item[\"metadata\"]\n",
    "\n",
    "        # Tokenisation\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Suppression de la dimension de batch ajoutée par le tokenizer (1, seq_len) -> (seq_len)\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Conversion des métadonnées en tenseur float\n",
    "        metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"metadata\": metadata_tensor\n",
    "        }\n",
    "\n",
    "        if self.with_labels:\n",
    "            label = item[\"label\"]\n",
    "            result[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70939522",
   "metadata": {
    "papermill": {
     "duration": 0.002772,
     "end_time": "2025-12-04T16:13:39.921413",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.918641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Architecture du Modèle Multimodal\n",
    "\n",
    "Le modèle combine un Transformer (pour le texte) et un MLP (pour les métadonnées)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84254db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:13:39.927893Z",
     "iopub.status.busy": "2025-12-04T16:13:39.927686Z",
     "iopub.status.idle": "2025-12-04T16:13:39.933354Z",
     "shell.execute_reply": "2025-12-04T16:13:39.932796Z"
    },
    "papermill": {
     "duration": 0.010241,
     "end_time": "2025-12-04T16:13:39.934405",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.924164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalTweetClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 transformer_name=\"cardiffnlp/twitter-xlm-roberta-base\",\n",
    "                 metadata_dim=8,\n",
    "                 text_hidden_dim=768,\n",
    "                 meta_hidden_dim=32,\n",
    "                 fusion_hidden_dim=256):\n",
    "        super(MultimodalTweetClassifier, self).__init__()\n",
    "        \n",
    "        # 1. Encodeur Transformer\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        \n",
    "        # 2. MLP pour les métadonnées\n",
    "        self.meta_mlp = nn.Sequential(\n",
    "            nn.Linear(metadata_dim, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(meta_hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # 3. Classifieur de fusion\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(text_hidden_dim + meta_hidden_dim, fusion_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(fusion_hidden_dim, 2)  # 2 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, metadata):\n",
    "        # Passage du texte dans le transformer\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Récupération du token CLS (premier token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Passage des métadonnées dans le MLP\n",
    "        meta_repr = self.meta_mlp(metadata)\n",
    "        \n",
    "        # Concaténation\n",
    "        fused = torch.cat([cls_embedding, meta_repr], dim=1)\n",
    "        \n",
    "        # Classification finale\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12b734",
   "metadata": {
    "papermill": {
     "duration": 0.00272,
     "end_time": "2025-12-04T16:13:39.939956",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.937236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Fonctions d'Entraînement et de Validation\n",
    "\n",
    "Fonctions utilitaires pour exécuter une époque d'entraînement et évaluer le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e48adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:13:39.946398Z",
     "iopub.status.busy": "2025-12-04T16:13:39.946183Z",
     "iopub.status.idle": "2025-12-04T16:13:39.952495Z",
     "shell.execute_reply": "2025-12-04T16:13:39.951984Z"
    },
    "papermill": {
     "duration": 0.010854,
     "end_time": "2025-12-04T16:13:39.953468",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.942614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        metadata = batch['metadata'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, metadata)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, metadata)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690adeb1",
   "metadata": {
    "papermill": {
     "duration": 0.002714,
     "end_time": "2025-12-04T16:13:39.958912",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.956198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Pipeline Principal d'Entraînement\n",
    "\n",
    "Lancement de l'entraînement avec séparation train/validation, initialisation du modèle et boucle d'époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c2fb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:13:39.965410Z",
     "iopub.status.busy": "2025-12-04T16:13:39.965219Z",
     "iopub.status.idle": "2025-12-04T19:02:37.032880Z",
     "shell.execute_reply": "2025-12-04T19:02:37.031898Z"
    },
    "papermill": {
     "duration": 10137.072477,
     "end_time": "2025-12-04T19:02:37.034098",
     "exception": false,
     "start_time": "2025-12-04T16:13:39.961621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 123931, Val size: 30983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-12-04 16:13:48.492100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764864828.678963      21 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764864828.727733      21 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'entraînement...\n",
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7746/7746 [39:29<00:00,  3.27it/s]\n",
      "Validation: 100%|██████████| 1937/1937 [02:38<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4109 | Val Loss: 0.3824 | Val Acc: 0.8300\n",
      "--> Nouveau meilleur modèle sauvegardé.\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7746/7746 [39:32<00:00,  3.26it/s]\n",
      "Validation: 100%|██████████| 1937/1937 [02:39<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4143 | Val Loss: 0.4162 | Val Acc: 0.8125\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7746/7746 [39:28<00:00,  3.27it/s]\n",
      "Validation: 100%|██████████| 1937/1937 [02:36<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4208 | Val Loss: 0.4132 | Val Acc: 0.8149\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7746/7746 [39:17<00:00,  3.29it/s]\n",
      "Validation: 100%|██████████| 1937/1937 [02:37<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4205 | Val Loss: 0.4126 | Val Acc: 0.8164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Vérification que les données sont chargées\n",
    "if 'train_data_processed' in locals():\n",
    "    # Séparation Train / Validation\n",
    "    train_list, val_list = train_test_split(\n",
    "        train_data_processed, \n",
    "        test_size=0.2, \n",
    "        random_state=cfg.seed, \n",
    "        stratify=[x['label'] for x in train_data_processed]\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_list)}, Val size: {len(val_list)}\")\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.transformer_name)\n",
    "\n",
    "    # Datasets & DataLoaders\n",
    "    train_dataset = TweetDataset(train_list, tokenizer, cfg.max_length, with_labels=True)\n",
    "    val_dataset = TweetDataset(val_list, tokenizer, cfg.max_length, with_labels=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # Initialisation du Modèle\n",
    "    model = MultimodalTweetClassifier(\n",
    "        transformer_name=cfg.transformer_name,\n",
    "        metadata_dim=cfg.metadata_dim,\n",
    "        text_hidden_dim=cfg.text_hidden_dim,\n",
    "        meta_hidden_dim=cfg.meta_hidden_dim,\n",
    "        fusion_hidden_dim=cfg.fusion_hidden_dim\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optionnel : Geler le transformer\n",
    "    if cfg.freeze_transformer:\n",
    "        print(\"Gel des paramètres du Transformer...\")\n",
    "        for p in model.transformer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Optimiseur\n",
    "    # On applique des learning rates différents pour le transformer et la tête de classification\n",
    "    transformer_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "    param_groups = []\n",
    "    if len(transformer_params) > 0:\n",
    "        param_groups.append({'params': transformer_params, 'lr': cfg.lr_transformer})\n",
    "    param_groups.append({'params': model.meta_mlp.parameters(), 'lr': cfg.lr_head})\n",
    "    param_groups.append({'params': model.classifier.parameters(), 'lr': cfg.lr_head})\n",
    "\n",
    "    optimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Boucle d'Entraînement\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(\"Début de l'entraînement...\")\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n",
    "            print(\"--> Nouveau meilleur modèle sauvegardé.\")\n",
    "else:\n",
    "    print(\"Données non chargées, impossible de lancer l'entraînement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5204eea",
   "metadata": {
    "papermill": {
     "duration": 1.502072,
     "end_time": "2025-12-04T19:02:40.112171",
     "exception": false,
     "start_time": "2025-12-04T19:02:38.610099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Inférence et Génération du Fichier de Soumission\n",
    "\n",
    "Chargement du meilleur modèle et prédiction sur le jeu de test Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2581fb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T19:02:42.983674Z",
     "iopub.status.busy": "2025-12-04T19:02:42.982911Z",
     "iopub.status.idle": "2025-12-04T19:11:25.533021Z",
     "shell.execute_reply": "2025-12-04T19:11:25.532246Z"
    },
    "papermill": {
     "duration": 523.924518,
     "end_time": "2025-12-04T19:11:25.534337",
     "exception": false,
     "start_time": "2025-12-04T19:02:41.609819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du meilleur modèle pour l'inférence...\n",
      "Lancement des prédictions sur le jeu de test Kaggle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 6462/6462 [08:41<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions sauvegardées dans submission.csv\n",
      "   ID  Prediction\n",
      "0   0           1\n",
      "1   2           1\n",
      "2   4           0\n",
      "3   8           1\n",
      "4   9           0\n"
     ]
    }
   ],
   "source": [
    "if 'kaggle_data_processed' in locals() and os.path.exists(\"best_multimodal_model.pt\"):\n",
    "    # Chargement du meilleur modèle\n",
    "    print(\"Chargement du meilleur modèle pour l'inférence...\")\n",
    "    model.load_state_dict(torch.load(\"best_multimodal_model.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Dataset Kaggle\n",
    "    kaggle_dataset = TweetDataset(kaggle_data_processed, tokenizer, cfg.max_length, with_labels=False)\n",
    "    kaggle_loader = DataLoader(kaggle_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    print(\"Lancement des prédictions sur le jeu de test Kaggle...\")\n",
    "    with torch.no_grad():\n",
    "        batch_start_idx = 0\n",
    "        for batch in tqdm(kaggle_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, metadata)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            \n",
    "            # Récupération des IDs correspondants\n",
    "            batch_size = input_ids.size(0)\n",
    "            batch_ids = [item['challenge_id'] for item in kaggle_data_processed[batch_start_idx : batch_start_idx + batch_size]]\n",
    "            all_ids.extend(batch_ids)\n",
    "            \n",
    "            batch_start_idx += batch_size\n",
    "\n",
    "    # Création du DataFrame de soumission\n",
    "    results_df = pd.DataFrame({\n",
    "        \"ID\": all_ids,\n",
    "        \"Prediction\": all_preds\n",
    "    })\n",
    "\n",
    "    output_file = \"submission.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Prédictions sauvegardées dans {output_file}\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"Impossible de lancer l'inférence (données manquantes ou modèle non entraîné).\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14382657,
     "sourceId": 120015,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 523570,
     "modelInstanceId": 508901,
     "sourceId": 671764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10846.650871,
   "end_time": "2025-12-04T19:11:30.039071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T16:10:43.388200",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
